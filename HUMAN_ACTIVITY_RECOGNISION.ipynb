{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activities are the class labels\n",
    "# It is a 6 class classification\n",
    "ACTIVITIES = {\n",
    "    1: 'WALKING',\n",
    "    2: 'WALKING_UPSTAIRS',\n",
    "    3: 'WALKING_DOWNSTAIRS',\n",
    "   \n",
    "    4: 'SITTING',#1\n",
    "    5: 'STANDING',\n",
    "    6: 'LAYING',\n",
    "}\n",
    "\n",
    "# Utility function to print the confusion matrix\n",
    "def confusion_matrix(Y_true, Y_pred):\n",
    "    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n",
    "    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n",
    "\n",
    "    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "plt.rcParams[\"font.family\"] = 'DejaVu Sans'\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory\n",
    "DATADIR = 'UCI_HAR_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data signals\n",
    "# Signals are from Accelerometer and Gyroscope\n",
    "# The signals are in x,y,z directions\n",
    "# Sensor signals are filtered to have only body acceleration\n",
    "# excluding the acceleration due to gravity\n",
    "# Triaxial acceleration from the accelerometer is total acceleration\n",
    "SIGNALS = [\n",
    "    \"body_acc_x\",\n",
    "    \"body_acc_y\",\n",
    "    \"body_acc_z\",\n",
    "    \"body_gyro_x\",\n",
    "    \"body_gyro_y\",\n",
    "    \"body_gyro_z\",\n",
    "    \"total_acc_x\",\n",
    "    \"total_acc_y\",\n",
    "    \"total_acc_z\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to read the data from csv file\n",
    "def _read_csv(filename):\n",
    "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "\n",
    "# Utility function to load the load\n",
    "def load_signals(subset):\n",
    "    signals_data = []\n",
    "\n",
    "    for signal in SIGNALS:\n",
    "        filename = f'UCI_HAR_Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
    "        signals_data.append(np.array(_read_csv(filename))) \n",
    "\n",
    "    # Transpose is used to change the dimensionality of the output,\n",
    "    # aggregating the signals by combination of sample/timestep.\n",
    "    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
    "    return np.transpose(signals_data, (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_y(subset):\n",
    "    \"\"\"\n",
    "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "    that represents a human activity. We return a binary representation of \n",
    "    every sample objective as a 6 bits vector using One Hot Encoding\n",
    "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "    \"\"\"\n",
    "    filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "    y = _read_csv(filename)[0]\n",
    "\n",
    "    return np.array(pd.get_dummies(y))\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Obtain the dataset from multiple files.\n",
    "    Returns: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test = load_signals('train'), load_signals('test')\n",
    "    y_train, y_test = load_y('train'), load_y('test')\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing tensorflow\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring a session\n",
    "session_conf = tf.compat.v1.ConfigProto(\n",
    "    intra_op_parallelism_threads=1,\n",
    "    inter_op_parallelism_threads=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Keras\n",
    "from keras import backend as K\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,MaxPooling1D,Flatten,BatchNormalization\n",
    "from keras.layers.core import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing parameters\n",
    "epochs = 30\n",
    "batch_size = 16\n",
    "n_hidden = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to count the number of classes\n",
    "def _count_classes(y):\n",
    "    return len(set([tuple(category) for category in y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the train and test data\n",
    "X_train, X_test, Y_train, Y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "9\n",
      "7352\n"
     ]
    }
   ],
   "source": [
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = _count_classes(Y_train)\n",
    "\n",
    "print(timesteps)\n",
    "print(input_dim)\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Defining the Architecture of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 32)                5376      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 5,574\n",
      "Trainable params: 5,574\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initiliazing the sequential model\n",
    "model = Sequential()\n",
    "# Configuring the parameters\n",
    "model.add(LSTM(n_hidden, input_shape=(timesteps, input_dim)))\n",
    "# Adding a dropout layer\n",
    "model.add(Dropout(0.5))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/30\n",
      "7352/7352 [==============================] - 92s 13ms/step - loss: 1.3018 - acc: 0.4395 - val_loss: 1.1254 - val_acc: 0.4662\n",
      "Epoch 2/30\n",
      "7352/7352 [==============================] - 94s 13ms/step - loss: 0.9666 - acc: 0.5880 - val_loss: 0.9491 - val_acc: 0.5714\n",
      "Epoch 3/30\n",
      "7352/7352 [==============================] - 97s 13ms/step - loss: 0.7812 - acc: 0.6408 - val_loss: 0.8286 - val_acc: 0.5850\n",
      "Epoch 4/30\n",
      "7352/7352 [==============================] - 95s 13ms/step - loss: 0.6941 - acc: 0.6574 - val_loss: 0.7297 - val_acc: 0.6128\n",
      "Epoch 5/30\n",
      "7352/7352 [==============================] - 92s 13ms/step - loss: 0.6336 - acc: 0.6912 - val_loss: 0.7359 - val_acc: 0.6787\n",
      "Epoch 6/30\n",
      "7352/7352 [==============================] - 94s 13ms/step - loss: 0.5859 - acc: 0.7134 - val_loss: 0.7015 - val_acc: 0.6939\n",
      "Epoch 7/30\n",
      "7352/7352 [==============================] - 95s 13ms/step - loss: 0.5692 - acc: 0.7477 - val_loss: 0.5995 - val_acc: 0.7387\n",
      "Epoch 8/30\n",
      "7352/7352 [==============================] - 96s 13ms/step - loss: 0.4899 - acc: 0.7809 - val_loss: 0.5762 - val_acc: 0.7387\n",
      "Epoch 9/30\n",
      "7352/7352 [==============================] - 90s 12ms/step - loss: 0.4482 - acc: 0.7886 - val_loss: 0.7413 - val_acc: 0.7126\n",
      "Epoch 10/30\n",
      "7352/7352 [==============================] - 90s 12ms/step - loss: 0.4132 - acc: 0.8077 - val_loss: 0.5048 - val_acc: 0.7513\n",
      "Epoch 11/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.3985 - acc: 0.8274 - val_loss: 0.5234 - val_acc: 0.7452\n",
      "Epoch 12/30\n",
      "7352/7352 [==============================] - 91s 12ms/step - loss: 0.3378 - acc: 0.8638 - val_loss: 0.4114 - val_acc: 0.8833\n",
      "Epoch 13/30\n",
      "7352/7352 [==============================] - 91s 12ms/step - loss: 0.2947 - acc: 0.9051 - val_loss: 0.4386 - val_acc: 0.8731\n",
      "Epoch 14/30\n",
      "7352/7352 [==============================] - 90s 12ms/step - loss: 0.2448 - acc: 0.9291 - val_loss: 0.3768 - val_acc: 0.8921\n",
      "Epoch 15/30\n",
      "7352/7352 [==============================] - 91s 12ms/step - loss: 0.2157 - acc: 0.9331 - val_loss: 0.4441 - val_acc: 0.8931\n",
      "Epoch 16/30\n",
      "7352/7352 [==============================] - 90s 12ms/step - loss: 0.2053 - acc: 0.9366 - val_loss: 0.4162 - val_acc: 0.8968\n",
      "Epoch 17/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.2028 - acc: 0.9404 - val_loss: 0.4538 - val_acc: 0.8962\n",
      "Epoch 18/30\n",
      "7352/7352 [==============================] - 93s 13ms/step - loss: 0.1911 - acc: 0.9419 - val_loss: 0.3964 - val_acc: 0.8999\n",
      "Epoch 19/30\n",
      "7352/7352 [==============================] - 96s 13ms/step - loss: 0.1912 - acc: 0.9407 - val_loss: 0.3165 - val_acc: 0.9030\n",
      "Epoch 20/30\n",
      "7352/7352 [==============================] - 96s 13ms/step - loss: 0.1732 - acc: 0.9446 - val_loss: 0.4546 - val_acc: 0.8904\n",
      "Epoch 21/30\n",
      "7352/7352 [==============================] - 94s 13ms/step - loss: 0.1782 - acc: 0.9444 - val_loss: 0.3346 - val_acc: 0.9063\n",
      "Epoch 22/30\n",
      "7352/7352 [==============================] - 95s 13ms/step - loss: 0.1812 - acc: 0.9418 - val_loss: 0.8164 - val_acc: 0.8582\n",
      "Epoch 23/30\n",
      "7352/7352 [==============================] - 95s 13ms/step - loss: 0.1824 - acc: 0.9426 - val_loss: 0.4240 - val_acc: 0.9036\n",
      "Epoch 24/30\n",
      "7352/7352 [==============================] - 94s 13ms/step - loss: 0.1726 - acc: 0.9429 - val_loss: 0.4067 - val_acc: 0.9148\n",
      "Epoch 25/30\n",
      "7352/7352 [==============================] - 96s 13ms/step - loss: 0.1737 - acc: 0.9411 - val_loss: 0.3396 - val_acc: 0.9074\n",
      "Epoch 26/30\n",
      "7352/7352 [==============================] - 96s 13ms/step - loss: 0.1650 - acc: 0.9461 - val_loss: 0.3806 - val_acc: 0.9019\n",
      "Epoch 27/30\n",
      "7352/7352 [==============================] - 89s 12ms/step - loss: 0.1925 - acc: 0.9415 - val_loss: 0.6464 - val_acc: 0.8850\n",
      "Epoch 28/30\n",
      "7352/7352 [==============================] - 91s 12ms/step - loss: 0.1965 - acc: 0.9425 - val_loss: 0.3363 - val_acc: 0.9203\n",
      "Epoch 29/30\n",
      "7352/7352 [==============================] - 92s 12ms/step - loss: 0.1889 - acc: 0.9431 - val_loss: 0.3737 - val_acc: 0.9158\n",
      "Epoch 30/30\n",
      "7352/7352 [==============================] - 95s 13ms/step - loss: 0.1945 - acc: 0.9414 - val_loss: 0.3088 - val_acc: 0.9097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29b5ee36a20>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred                LAYING  SITTING  STANDING  WALKING  WALKING_DOWNSTAIRS  \\\n",
      "True                                                                         \n",
      "LAYING                 512        0        25        0                   0   \n",
      "SITTING                  3      410        75        0                   0   \n",
      "STANDING                 0       87       445        0                   0   \n",
      "WALKING                  0        0         0      481                   2   \n",
      "WALKING_DOWNSTAIRS       0        0         0        0                 382   \n",
      "WALKING_UPSTAIRS         0        0         0        2                  18   \n",
      "\n",
      "Pred                WALKING_UPSTAIRS  \n",
      "True                                  \n",
      "LAYING                             0  \n",
      "SITTING                            3  \n",
      "STANDING                           0  \n",
      "WALKING                           13  \n",
      "WALKING_DOWNSTAIRS                38  \n",
      "WALKING_UPSTAIRS                 451  \n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(Y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2947/2947 [==============================] - 4s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3087582236972612, 0.9097387173396675]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With a simple 2 layer architecture we got 90.09% accuracy and a loss of 0.30\n",
    "- We can further imporve the performace with Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Just changing dropout and layers of Lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing parameters\n",
    "tf.keras.backend.clear_session()\n",
    "epochs = 30\n",
    "batch_size = 16\n",
    "n_hidden = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 25)                3500      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 156       \n",
      "=================================================================\n",
      "Total params: 3,656\n",
      "Trainable params: 3,656\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initiliazing the sequential model\n",
    "#n_hidden=25,dropout=0.275\n",
    "model = Sequential()\n",
    "# Configuring the parameters\n",
    "model.add(LSTM(n_hidden, input_shape=(timesteps, input_dim)))\n",
    "# Adding a dropout layer\n",
    "model.add(Dropout(0.275))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 1.3714 - accuracy: 0.4138 - val_loss: 1.2028 - val_accuracy: 0.4496\n",
      "Epoch 2/30\n",
      "460/460 [==============================] - 5s 12ms/step - loss: 1.0548 - accuracy: 0.5487 - val_loss: 1.0137 - val_accuracy: 0.5307\n",
      "Epoch 3/30\n",
      "460/460 [==============================] - 5s 12ms/step - loss: 0.8974 - accuracy: 0.6185 - val_loss: 0.8855 - val_accuracy: 0.5969\n",
      "Epoch 4/30\n",
      "460/460 [==============================] - 5s 12ms/step - loss: 0.7789 - accuracy: 0.6692 - val_loss: 0.7431 - val_accuracy: 0.7000\n",
      "Epoch 5/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.7090 - accuracy: 0.6876 - val_loss: 0.7831 - val_accuracy: 0.6569\n",
      "Epoch 6/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.6622 - accuracy: 0.7085 - val_loss: 0.6781 - val_accuracy: 0.7146\n",
      "Epoch 7/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.5510 - accuracy: 0.7652 - val_loss: 0.5785 - val_accuracy: 0.7401\n",
      "Epoch 8/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.5171 - accuracy: 0.7971 - val_loss: 0.5253 - val_accuracy: 0.7900\n",
      "Epoch 9/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.5003 - accuracy: 0.8268 - val_loss: 0.9748 - val_accuracy: 0.7204\n",
      "Epoch 10/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.4536 - accuracy: 0.8413 - val_loss: 0.5037 - val_accuracy: 0.8354\n",
      "Epoch 11/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.4245 - accuracy: 0.8557 - val_loss: 0.5016 - val_accuracy: 0.8307\n",
      "Epoch 12/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.3221 - accuracy: 0.8953 - val_loss: 0.4368 - val_accuracy: 0.8612\n",
      "Epoch 13/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.2727 - accuracy: 0.9117 - val_loss: 0.3951 - val_accuracy: 0.8646\n",
      "Epoch 14/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.2446 - accuracy: 0.9261 - val_loss: 0.3467 - val_accuracy: 0.8873\n",
      "Epoch 15/30\n",
      "460/460 [==============================] - 5s 12ms/step - loss: 0.2289 - accuracy: 0.9252 - val_loss: 0.4105 - val_accuracy: 0.8802\n",
      "Epoch 16/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.2267 - accuracy: 0.9294 - val_loss: 0.3990 - val_accuracy: 0.8758\n",
      "Epoch 17/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.1924 - accuracy: 0.9382 - val_loss: 0.2874 - val_accuracy: 0.8985\n",
      "Epoch 18/30\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1857 - accuracy: 0.9377 - val_loss: 0.2790 - val_accuracy: 0.8962\n",
      "Epoch 19/30\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1782 - accuracy: 0.9408 - val_loss: 0.2764 - val_accuracy: 0.9101\n",
      "Epoch 20/30\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 0.1810 - accuracy: 0.9411 - val_loss: 0.2496 - val_accuracy: 0.9203\n",
      "Epoch 21/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.1944 - accuracy: 0.9366 - val_loss: 0.2909 - val_accuracy: 0.9111\n",
      "Epoch 22/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.1750 - accuracy: 0.9408 - val_loss: 0.2551 - val_accuracy: 0.9063\n",
      "Epoch 23/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.1514 - accuracy: 0.9476 - val_loss: 0.2570 - val_accuracy: 0.9104\n",
      "Epoch 24/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.1616 - accuracy: 0.9460 - val_loss: 0.2289 - val_accuracy: 0.9101\n",
      "Epoch 25/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.1524 - accuracy: 0.9453 - val_loss: 0.2779 - val_accuracy: 0.9111\n",
      "Epoch 26/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.1591 - accuracy: 0.9453 - val_loss: 0.3397 - val_accuracy: 0.9040\n",
      "Epoch 27/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.1540 - accuracy: 0.9470 - val_loss: 0.2965 - val_accuracy: 0.9152\n",
      "Epoch 28/30\n",
      "460/460 [==============================] - 5s 12ms/step - loss: 0.1502 - accuracy: 0.9479 - val_loss: 0.2495 - val_accuracy: 0.9226\n",
      "Epoch 29/30\n",
      "460/460 [==============================] - 5s 12ms/step - loss: 0.1451 - accuracy: 0.9480 - val_loss: 0.2523 - val_accuracy: 0.9220\n",
      "Epoch 30/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.1415 - accuracy: 0.9489 - val_loss: 0.2658 - val_accuracy: 0.9216\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b9ac13d708>"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred                LAYING  SITTING  STANDING  WALKING  WALKING_DOWNSTAIRS  \\\n",
      "True                                                                         \n",
      "LAYING                 537        0         0        0                   0   \n",
      "SITTING                  2      400        64        1                   0   \n",
      "STANDING                 0       96       430        6                   0   \n",
      "WALKING                  0        0         0      480                  15   \n",
      "WALKING_DOWNSTAIRS       0        0         0        0                 416   \n",
      "WALKING_UPSTAIRS         0        1         0        9                   8   \n",
      "\n",
      "Pred                WALKING_UPSTAIRS  \n",
      "True                                  \n",
      "LAYING                             0  \n",
      "SITTING                           24  \n",
      "STANDING                           0  \n",
      "WALKING                            1  \n",
      "WALKING_DOWNSTAIRS                 4  \n",
      "WALKING_UPSTAIRS                 453  \n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(confusion_matrix(Y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22888034582138062"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(model.history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.922633171081543"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(model.history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#tried single layer 2 with different params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing parameters\n",
    "tf.keras.backend.clear_session()\n",
    "epochs = 30\n",
    "batch_size = 16\n",
    "n_hidden = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 25)                3500      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 156       \n",
      "=================================================================\n",
      "Total params: 3,656\n",
      "Trainable params: 3,656\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initiliazing the sequential model\n",
    "#n_hidden=25,dropout=0.275\n",
    "model = Sequential()\n",
    "# Configuring the parameters\n",
    "model.add(LSTM(n_hidden, input_shape=(timesteps, input_dim)))\n",
    "# Adding a dropout layer\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "460/460 [==============================] - 6s 13ms/step - loss: 1.4020 - accuracy: 0.3993 - val_loss: 1.3000 - val_accuracy: 0.4018\n",
      "Epoch 2/30\n",
      "460/460 [==============================] - 5s 12ms/step - loss: 1.1845 - accuracy: 0.4701 - val_loss: 1.2811 - val_accuracy: 0.4432\n",
      "Epoch 3/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 1.1132 - accuracy: 0.4859 - val_loss: 1.1187 - val_accuracy: 0.4645\n",
      "Epoch 4/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.9664 - accuracy: 0.5471 - val_loss: 0.9668 - val_accuracy: 0.5124\n",
      "Epoch 5/30\n",
      "460/460 [==============================] - 5s 12ms/step - loss: 0.8531 - accuracy: 0.6209 - val_loss: 0.9379 - val_accuracy: 0.6064\n",
      "Epoch 6/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.7776 - accuracy: 0.6658 - val_loss: 0.8319 - val_accuracy: 0.6247\n",
      "Epoch 7/30\n",
      "460/460 [==============================] - 5s 12ms/step - loss: 0.7275 - accuracy: 0.6620 - val_loss: 0.7468 - val_accuracy: 0.6169\n",
      "Epoch 8/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.6665 - accuracy: 0.6865 - val_loss: 0.7485 - val_accuracy: 0.6983\n",
      "Epoch 9/30\n",
      "460/460 [==============================] - 5s 12ms/step - loss: 0.6453 - accuracy: 0.6922 - val_loss: 0.7320 - val_accuracy: 0.7085\n",
      "Epoch 10/30\n",
      "460/460 [==============================] - 5s 12ms/step - loss: 0.6243 - accuracy: 0.6946 - val_loss: 0.7374 - val_accuracy: 0.6804\n",
      "Epoch 11/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.6044 - accuracy: 0.7091 - val_loss: 0.7384 - val_accuracy: 0.6885\n",
      "Epoch 12/30\n",
      "460/460 [==============================] - 5s 12ms/step - loss: 0.6513 - accuracy: 0.6906 - val_loss: 0.8060 - val_accuracy: 0.7065\n",
      "Epoch 13/30\n",
      "460/460 [==============================] - 5s 12ms/step - loss: 0.5981 - accuracy: 0.7159 - val_loss: 0.7063 - val_accuracy: 0.7014\n",
      "Epoch 14/30\n",
      "460/460 [==============================] - 5s 12ms/step - loss: 0.5802 - accuracy: 0.7205 - val_loss: 0.7846 - val_accuracy: 0.6573\n",
      "Epoch 15/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.6000 - accuracy: 0.6999 - val_loss: 0.8223 - val_accuracy: 0.6753\n",
      "Epoch 16/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.5621 - accuracy: 0.7229 - val_loss: 0.7025 - val_accuracy: 0.6905\n",
      "Epoch 17/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.5399 - accuracy: 0.7237 - val_loss: 0.6955 - val_accuracy: 0.6698\n",
      "Epoch 18/30\n",
      "460/460 [==============================] - 5s 12ms/step - loss: 0.5268 - accuracy: 0.7363 - val_loss: 0.7190 - val_accuracy: 0.6804\n",
      "Epoch 19/30\n",
      "460/460 [==============================] - 5s 12ms/step - loss: 0.5006 - accuracy: 0.7635 - val_loss: 0.7401 - val_accuracy: 0.7601\n",
      "Epoch 20/30\n",
      "460/460 [==============================] - 5s 12ms/step - loss: 0.4986 - accuracy: 0.7870 - val_loss: 0.7140 - val_accuracy: 0.7764\n",
      "Epoch 21/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.4513 - accuracy: 0.8158 - val_loss: 0.6508 - val_accuracy: 0.7733\n",
      "Epoch 22/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.4274 - accuracy: 0.8191 - val_loss: 0.6790 - val_accuracy: 0.7659\n",
      "Epoch 23/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.4140 - accuracy: 0.8162 - val_loss: 0.6941 - val_accuracy: 0.7625\n",
      "Epoch 24/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.4025 - accuracy: 0.8239 - val_loss: 0.6430 - val_accuracy: 0.7618\n",
      "Epoch 25/30\n",
      "460/460 [==============================] - 5s 12ms/step - loss: 0.3722 - accuracy: 0.8573 - val_loss: 0.7091 - val_accuracy: 0.8039\n",
      "Epoch 26/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.3900 - accuracy: 0.8870 - val_loss: 0.7245 - val_accuracy: 0.8314\n",
      "Epoch 27/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.3071 - accuracy: 0.9082 - val_loss: 0.5824 - val_accuracy: 0.8534\n",
      "Epoch 28/30\n",
      "460/460 [==============================] - 5s 12ms/step - loss: 0.2814 - accuracy: 0.9200 - val_loss: 0.5518 - val_accuracy: 0.8548\n",
      "Epoch 29/30\n",
      "460/460 [==============================] - 6s 12ms/step - loss: 0.2766 - accuracy: 0.9183 - val_loss: 0.5477 - val_accuracy: 0.8561\n",
      "Epoch 30/30\n",
      "460/460 [==============================] - 5s 12ms/step - loss: 0.2615 - accuracy: 0.9240 - val_loss: 0.5650 - val_accuracy: 0.8605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ba5041d7c8>"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9151679873466492"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(model.history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2559111416339874"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(model.history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying multi layer LSTMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing parameters\n",
    "tf.keras.backend.clear_session()\n",
    "epochs = 30\n",
    "batch_size = 16\n",
    "n_hidden1 = 32\n",
    "n_hidden2 = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 128, 32)           5376      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 25)                5800      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 25)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 156       \n",
      "=================================================================\n",
      "Total params: 11,332\n",
      "Trainable params: 11,332\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Initializing parameters\n",
    "tf.keras.backend.clear_session()\n",
    "epochs = 30\n",
    "batch_size = 16\n",
    "n_hidden1 = 32\n",
    "n_hidden2 = 25\n",
    "# Initiliazing the sequential model\n",
    "model = Sequential()\n",
    "# Configuring the parameters\n",
    "model.add(LSTM(n_hidden1, return_sequences=True,input_shape=(timesteps, input_dim)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(n_hidden2,  input_shape=(timesteps, input_dim)))\n",
    "\n",
    "# Adding a dropout layer\n",
    "model.add(Dropout(0.275))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "checkpoint_path = \"training_model_Lstm2/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,save_best_only=True,save_weights_only=True,verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "460/460 [==============================] - 11s 23ms/step - loss: 0.1235 - accuracy: 0.9554 - val_loss: 0.5479 - val_accuracy: 0.8999\n",
      "Epoch 2/10\n",
      "460/460 [==============================] - 11s 23ms/step - loss: 0.1349 - accuracy: 0.9538 - val_loss: 0.3872 - val_accuracy: 0.9281\n",
      "Epoch 3/10\n",
      "460/460 [==============================] - 10s 23ms/step - loss: 0.1278 - accuracy: 0.9535 - val_loss: 0.4585 - val_accuracy: 0.9114\n",
      "Epoch 4/10\n",
      "460/460 [==============================] - 10s 23ms/step - loss: 0.1282 - accuracy: 0.9543 - val_loss: 0.3699 - val_accuracy: 0.9125\n",
      "Epoch 5/10\n",
      "460/460 [==============================] - 10s 23ms/step - loss: 0.1211 - accuracy: 0.9542 - val_loss: 0.3578 - val_accuracy: 0.9267\n",
      "Epoch 6/10\n",
      "460/460 [==============================] - 10s 22ms/step - loss: 0.1198 - accuracy: 0.9563 - val_loss: 0.3725 - val_accuracy: 0.9213\n",
      "Epoch 7/10\n",
      "460/460 [==============================] - 10s 22ms/step - loss: 0.1306 - accuracy: 0.9518 - val_loss: 0.4280 - val_accuracy: 0.9206\n",
      "Epoch 8/10\n",
      "460/460 [==============================] - 10s 23ms/step - loss: 0.1295 - accuracy: 0.9546 - val_loss: 0.3652 - val_accuracy: 0.9287\n",
      "Epoch 9/10\n",
      "460/460 [==============================] - 10s 23ms/step - loss: 0.1266 - accuracy: 0.9551 - val_loss: 0.3702 - val_accuracy: 0.9108\n",
      "Epoch 10/10\n",
      "460/460 [==============================] - 10s 22ms/step - loss: 0.1378 - accuracy: 0.9494 - val_loss: 0.4756 - val_accuracy: 0.9057\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x223c539bd88>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1fd6e253b08>"
      ]
     },
     "execution_count": 500,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"training_model_lstm/cp.ckpt\"\n",
    "model_ckpt2 = model\n",
    "model_ckpt2.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after changing many dropouts andd no of hidden layer\n",
    "#for the first model we got 0.9287 as highest accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIVIDE and CONQUER APPROch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.researchgate.net/publication/324224939_Divide_and_Conquer-Based_1D_CNN_Human_Activity_Recognition_Using_Test_Data_Sharpening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#dividing static and dynamic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_y2(subset):\n",
    "    \"\"\"\n",
    "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "    that represents a human activity. We return a binary representation of \n",
    "    every sample objective as a 6 bits vector using One Hot Encoding\n",
    "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "    \"\"\"\n",
    "    filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "    y = _read_csv(filename)[0]\n",
    "    y[y<=3] = 1\n",
    "    y[y>3] = 0\n",
    "\n",
    "    return np.array(pd.get_dummies(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 4, 6, 1, 3, 2], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = f'UCI_HAR_Dataset/test/y_test.txt'\n",
    "y = _read_csv(filename)[0]\n",
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    " #   1: 'WALKING',\n",
    "  #  2: 'WALKING_UPSTAIRS',\n",
    "   # 3: 'WALKING_DOWNSTAIRS',\n",
    "    #4: 'SITTING',\n",
    "    #5: 'STANDING',\n",
    "    #6: 'LAYING',\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Obtain the dataset from multiple files.\n",
    "    Returns: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test = load_signals('train'), load_signals('test')\n",
    "    y_train, y_test = load_y2('train'), load_y2('test')\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the train and test data\n",
    "X_train, X_test, Y_train, Y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "9\n",
      "7352\n"
     ]
    }
   ],
   "source": [
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = _count_classes(Y_train)\n",
    "\n",
    "print(timesteps)\n",
    "print(input_dim)\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7352, 2)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 128, 25)           3500      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128, 25)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 12)                1824      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 78        \n",
      "=================================================================\n",
      "Total params: 5,402\n",
      "Trainable params: 5,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "epochs = 30\n",
    "batch_size = 16\n",
    "n_hidden1= 25\n",
    "n_hidden2 = 12\n",
    "# Initiliazing the sequential model\n",
    "model = Sequential()\n",
    "# Configuring the parameters\n",
    "#model.add(Conv1D(filters=32, kernel_size=3, activation='relu',kernel_initializer='he_uniform'))\n",
    "\n",
    "model.add(LSTM(n_hidden1, input_shape=(timesteps, input_dim),return_sequences=True))\n",
    "# Adding a dropout layer\n",
    "model.add(Dropout(0.375))\n",
    "model.add(LSTM(n_hidden2, input_shape=(timesteps, input_dim)))\n",
    "# Adding a dropout layer\n",
    "model.add(Dropout(0.375))\n",
    "# Adding a dense output layer with sigmoid activation\n",
    "model.add(Dense(n_classes, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "checkpoint_path = \"training_model_2class/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,save_best_only=True,save_weights_only=True,verbose=1)\n",
    "\n",
    "#checkpoint_path = \"training_model_2class/cp.ckpt\"\n",
    "#model_ckpt2 = model2\n",
    "#model_ckpt2.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "460/460 [==============================] - ETA: 0s - loss: 0.2574 - accuracy: 0.9144\n",
      "Epoch 00001: val_loss improved from inf to 0.10749, saving model to training_model_2class/cp.ckpt\n",
      "460/460 [==============================] - 10s 21ms/step - loss: 0.2574 - accuracy: 0.9144 - val_loss: 0.1075 - val_accuracy: 0.9691\n",
      "Epoch 2/10\n",
      "457/460 [============================>.] - ETA: 0s - loss: 0.0513 - accuracy: 0.9902 ETA: 0s - loss: 0.0498 - accu\n",
      "Epoch 00002: val_loss improved from 0.10749 to 0.07481, saving model to training_model_2class/cp.ckpt\n",
      "460/460 [==============================] - 9s 19ms/step - loss: 0.0511 - accuracy: 0.9902 - val_loss: 0.0748 - val_accuracy: 0.9793\n",
      "Epoch 3/10\n",
      "458/460 [============================>.] - ETA: 0s - loss: 0.0667 - accuracy: 0.9858 ETA: 0s -\n",
      "Epoch 00003: val_loss did not improve from 0.07481\n",
      "460/460 [==============================] - 9s 19ms/step - loss: 0.0666 - accuracy: 0.9859 - val_loss: 0.0811 - val_accuracy: 0.9796\n",
      "Epoch 4/10\n",
      "460/460 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 0.9939\n",
      "Epoch 00004: val_loss improved from 0.07481 to 0.02549, saving model to training_model_2class/cp.ckpt\n",
      "460/460 [==============================] - 9s 19ms/step - loss: 0.0366 - accuracy: 0.9939 - val_loss: 0.0255 - val_accuracy: 0.9942\n",
      "Epoch 5/10\n",
      "460/460 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9901\n",
      "Epoch 00005: val_loss did not improve from 0.02549\n",
      "460/460 [==============================] - 9s 19ms/step - loss: 0.0478 - accuracy: 0.9901 - val_loss: 0.0256 - val_accuracy: 0.9912\n",
      "Epoch 6/10\n",
      "458/460 [============================>.] - ETA: 0s - loss: 0.0178 - accuracy: 0.9971\n",
      "Epoch 00006: val_loss improved from 0.02549 to 0.00969, saving model to training_model_2class/cp.ckpt\n",
      "460/460 [==============================] - 9s 19ms/step - loss: 0.0177 - accuracy: 0.9971 - val_loss: 0.0097 - val_accuracy: 0.9980\n",
      "Epoch 7/10\n",
      "458/460 [============================>.] - ETA: 0s - loss: 0.0091 - accuracy: 0.9989\n",
      "Epoch 00007: val_loss did not improve from 0.00969\n",
      "460/460 [==============================] - 9s 19ms/step - loss: 0.0091 - accuracy: 0.9989 - val_loss: 0.0103 - val_accuracy: 0.9980\n",
      "Epoch 8/10\n",
      "460/460 [==============================] - ETA: 0s - loss: 0.0072 - accuracy: 0.9989\n",
      "Epoch 00008: val_loss improved from 0.00969 to 0.00634, saving model to training_model_2class/cp.ckpt\n",
      "460/460 [==============================] - 9s 19ms/step - loss: 0.0072 - accuracy: 0.9989 - val_loss: 0.0063 - val_accuracy: 0.9983\n",
      "Epoch 9/10\n",
      "459/460 [============================>.] - ETA: 0s - loss: 0.0041 - accuracy: 0.9996\n",
      "Epoch 00009: val_loss did not improve from 0.00634\n",
      "460/460 [==============================] - 9s 19ms/step - loss: 0.0041 - accuracy: 0.9996 - val_loss: 0.0101 - val_accuracy: 0.9980\n",
      "Epoch 10/10\n",
      "457/460 [============================>.] - ETA: 0s - loss: 0.0207 - accuracy: 0.9964\n",
      "Epoch 00010: val_loss did not improve from 0.00634\n",
      "460/460 [==============================] - 9s 19ms/step - loss: 0.0206 - accuracy: 0.9965 - val_loss: 0.0089 - val_accuracy: 0.9976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2c715dce088>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling the model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',metrics=['accuracy'])\n",
    "# Training the model\n",
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=10,callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "457/460 [============================>.] - ETA: 0s - loss: 0.0109 - accuracy: 0.9979\n",
      "Epoch 00001: val_loss did not improve from 0.00634\n",
      "460/460 [==============================] - 9s 19ms/step - loss: 0.0109 - accuracy: 0.9980 - val_loss: 0.0104 - val_accuracy: 0.9980\n",
      "Epoch 2/5\n",
      "458/460 [============================>.] - ETA: 0s - loss: 0.0042 - accuracy: 0.9995\n",
      "Epoch 00002: val_loss improved from 0.00634 to 0.00471, saving model to training_model_2class/cp.ckpt\n",
      "460/460 [==============================] - 9s 19ms/step - loss: 0.0042 - accuracy: 0.9995 - val_loss: 0.0047 - val_accuracy: 0.9990\n",
      "Epoch 3/5\n",
      "460/460 [==============================] - ETA: 0s - loss: 0.0053 - accuracy: 0.9992\n",
      "Epoch 00003: val_loss did not improve from 0.00471\n",
      "460/460 [==============================] - 9s 19ms/step - loss: 0.0053 - accuracy: 0.9992 - val_loss: 0.0071 - val_accuracy: 0.9983\n",
      "Epoch 4/5\n",
      "458/460 [============================>.] - ETA: 0s - loss: 0.0017 - accuracy: 0.9999\n",
      "Epoch 00004: val_loss did not improve from 0.00471\n",
      "460/460 [==============================] - 9s 19ms/step - loss: 0.0017 - accuracy: 0.9999 - val_loss: 0.0089 - val_accuracy: 0.9980\n",
      "Epoch 5/5\n",
      "459/460 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9997\n",
      "Epoch 00005: val_loss improved from 0.00471 to 0.00441, saving model to training_model_2class/cp.ckpt\n",
      "460/460 [==============================] - 9s 20ms/step - loss: 0.0037 - accuracy: 0.9997 - val_loss: 0.0044 - val_accuracy: 0.9990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2c777128e08>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=5,callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "460/460 [==============================] - ETA: 0s - loss: 9.3469e-04 - accuracy: 1.0000\n",
      "Epoch 00001: val_loss did not improve from 0.00441\n",
      "460/460 [==============================] - 9s 19ms/step - loss: 9.3469e-04 - accuracy: 1.0000 - val_loss: 0.0063 - val_accuracy: 0.9990\n",
      "Epoch 2/5\n",
      "458/460 [============================>.] - ETA: 0s - loss: 6.1602e-04 - accuracy: 1.0000\n",
      "Epoch 00002: val_loss did not improve from 0.00441\n",
      "460/460 [==============================] - 9s 19ms/step - loss: 6.1579e-04 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 0.9976\n",
      "Epoch 3/5\n",
      "458/460 [============================>.] - ETA: 0s - loss: 6.6885e-04 - accuracy: 1.0000\n",
      "Epoch 00003: val_loss improved from 0.00441 to 0.00180, saving model to training_model_2class/cp.ckpt\n",
      "460/460 [==============================] - 9s 19ms/step - loss: 6.6920e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 0.9993\n",
      "Epoch 4/5\n",
      "457/460 [============================>.] - ETA: 0s - loss: 4.7915e-04 - accuracy: 1.0000\n",
      "Epoch 00004: val_loss did not improve from 0.00180\n",
      "460/460 [==============================] - 9s 19ms/step - loss: 4.7957e-04 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 0.9993\n",
      "Epoch 5/5\n",
      "460/460 [==============================] - ETA: 0s - loss: 3.9312e-04 - accuracy: 1.0000\n",
      "Epoch 00005: val_loss did not improve from 0.00180\n",
      "460/460 [==============================] - 9s 20ms/step - loss: 3.9312e-04 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 0.9997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2c72c86ee48>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=5,callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9996606707572937"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(model.history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0017972142668440938"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(model.history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1fd1ac11488>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"training_model_2class/cp.ckpt\"\n",
    "model_1_2class = model1\n",
    "model_1_2class.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_2class.save('model_1_2class.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Here We have classified our model almost idealy to static and Dynamic with an accuracy of 99.96%\n",
    "#we used only two Lstm to achieve this high accuracy\n",
    "#and saved the best minal loos and saved as model 1 for future use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classifying static activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{\n",
    "#    4: 'SITTING',\n",
    "#    5: 'STANDING',\n",
    "#    6: 'LAYING',\n",
    "#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'UCI_HAR_Dataset/test/y_test.txt'\n",
    "y = _read_csv(filename)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stat(subset):\n",
    "    \"\"\"\n",
    "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "    that represents a human activity. We return a binary representation of \n",
    "    every sample objective as a 6 bits vector using One Hot Encoding\n",
    "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "    \"\"\"\n",
    "    filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "    y = _read_csv(filename)[0]\n",
    "    y_ind=y>3\n",
    "    Y=y[y_ind]\n",
    "\n",
    "    return np.array(pd.get_dummies(Y)),y_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Obtain the dataset from multiple files.\n",
    "    Returns: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test = load_signals('train'), load_signals('test')\n",
    "    y_train,xtr_stat=load_stat('train')\n",
    "    y_test,xte_stat=load_stat('test')\n",
    "    X_train_st=X_train[xtr_stat]\n",
    "    X_test_st=X_test[xte_stat]\n",
    "\n",
    "    return X_train_st, X_test_st, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the train and test data\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4067, 128, 9) (1560, 128, 9) (4067, 3) (1560, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,X_test.shape,Y_train.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "9\n",
      "4067\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = _count_classes(Y_train)\n",
    "\n",
    "print(timesteps)\n",
    "print(input_dim)\n",
    "print(len(X_train))\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 128, 100)          44000     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 128, 64)           19264     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 128, 32)           6176      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128, 32)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128, 25)           825       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128, 25)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 127, 25)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3175)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 9528      \n",
      "=================================================================\n",
      "Total params: 79,793\n",
      "Trainable params: 79,793\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "n_hidden1=100\n",
    "\n",
    "model_2 = Sequential()\n",
    "model_2.add(LSTM(n_hidden1,return_sequences=True, input_shape=(timesteps, input_dim)))\n",
    "model_2.add(Dropout(0.65))\n",
    "model_2.add(Conv1D(filters=64, kernel_size=3,padding='same', activation='relu',kernel_initializer='he_uniform'))\n",
    "model_2.add(Dropout(0.575))\n",
    "model_2.add(Conv1D(filters=32, kernel_size=3,padding='same', activation='relu',kernel_initializer='he_uniform'))\n",
    "model_2.add(Dropout(0.5))\n",
    "\n",
    "model_2.add(Dense(25, activation='relu',kernel_initializer='he_uniform'))\n",
    "model_2.add(Dropout(0.275))\n",
    "model_2.add(MaxPooling1D(pool_size=2,strides=1))\n",
    "model_2.add(Flatten())\n",
    "#model_2.add(Dense(32, activation='relu',kernel_initializer='he_uniform'))\n",
    "#model_2.add(BatchNormalization()) \n",
    "#model_2.add(Dropout(0.6))\n",
    "\n",
    "model_2.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "checkpoint_path = \"training_model2_stclass/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,save_best_only=True,save_weights_only=True,verbose=1)\n",
    "\n",
    "#checkpoint_path = \"training_model2_stclass/cp.ckpt\"\n",
    "#model_ckpt2 = model2\n",
    "#model_ckpt2.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "255/255 [==============================] - ETA: 0s - loss: 0.3732 - accuracy: 0.8264\n",
      "Epoch 00001: val_loss improved from inf to 0.49978, saving model to training_model2_stclass/cp.ckpt\n",
      "255/255 [==============================] - 7s 26ms/step - loss: 0.3732 - accuracy: 0.8264 - val_loss: 0.4998 - val_accuracy: 0.8654\n",
      "Epoch 2/10\n",
      "255/255 [==============================] - ETA: 0s - loss: 0.2473 - accuracy: 0.9012\n",
      "Epoch 00002: val_loss did not improve from 0.49978\n",
      "255/255 [==============================] - 6s 25ms/step - loss: 0.2473 - accuracy: 0.9012 - val_loss: 0.7465 - val_accuracy: 0.8250\n",
      "Epoch 3/10\n",
      "254/255 [============================>.] - ETA: 0s - loss: 0.2330 - accuracy: 0.9028\n",
      "Epoch 00003: val_loss did not improve from 0.49978\n",
      "255/255 [==============================] - 6s 25ms/step - loss: 0.2328 - accuracy: 0.9029 - val_loss: 0.6498 - val_accuracy: 0.8590\n",
      "Epoch 4/10\n",
      "253/255 [============================>.] - ETA: 0s - loss: 0.2154 - accuracy: 0.9106\n",
      "Epoch 00004: val_loss improved from 0.49978 to 0.34453, saving model to training_model2_stclass/cp.ckpt\n",
      "255/255 [==============================] - 6s 25ms/step - loss: 0.2159 - accuracy: 0.9103 - val_loss: 0.3445 - val_accuracy: 0.8231\n",
      "Epoch 5/10\n",
      "253/255 [============================>.] - ETA: 0s - loss: 0.2147 - accuracy: 0.9108\n",
      "Epoch 00005: val_loss improved from 0.34453 to 0.28377, saving model to training_model2_stclass/cp.ckpt\n",
      "255/255 [==============================] - 7s 26ms/step - loss: 0.2148 - accuracy: 0.9107 - val_loss: 0.2838 - val_accuracy: 0.8910\n",
      "Epoch 6/10\n",
      "253/255 [============================>.] - ETA: 0s - loss: 0.2006 - accuracy: 0.9098\n",
      "Epoch 00006: val_loss improved from 0.28377 to 0.27929, saving model to training_model2_stclass/cp.ckpt\n",
      "255/255 [==============================] - 6s 25ms/step - loss: 0.2001 - accuracy: 0.9103 - val_loss: 0.2793 - val_accuracy: 0.8878\n",
      "Epoch 7/10\n",
      "254/255 [============================>.] - ETA: 0s - loss: 0.2096 - accuracy: 0.9178\n",
      "Epoch 00007: val_loss did not improve from 0.27929\n",
      "255/255 [==============================] - 7s 26ms/step - loss: 0.2095 - accuracy: 0.9179 - val_loss: 0.2890 - val_accuracy: 0.9154\n",
      "Epoch 8/10\n",
      "254/255 [============================>.] - ETA: 0s - loss: 0.1984 - accuracy: 0.9240\n",
      "Epoch 00008: val_loss improved from 0.27929 to 0.25852, saving model to training_model2_stclass/cp.ckpt\n",
      "255/255 [==============================] - 7s 27ms/step - loss: 0.1982 - accuracy: 0.9240 - val_loss: 0.2585 - val_accuracy: 0.9141\n",
      "Epoch 9/10\n",
      "253/255 [============================>.] - ETA: 0s - loss: 0.1812 - accuracy: 0.9247\n",
      "Epoch 00009: val_loss did not improve from 0.25852\n",
      "255/255 [==============================] - 6s 25ms/step - loss: 0.1815 - accuracy: 0.9243 - val_loss: 0.3411 - val_accuracy: 0.9038\n",
      "Epoch 10/10\n",
      "254/255 [============================>.] - ETA: 0s - loss: 0.1856 - accuracy: 0.9299\n",
      "Epoch 00010: val_loss did not improve from 0.25852\n",
      "255/255 [==============================] - 6s 25ms/step - loss: 0.1854 - accuracy: 0.9299 - val_loss: 0.3458 - val_accuracy: 0.8917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fd929a8c48>"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling the model\n",
    "model_2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',metrics=['accuracy'])\n",
    "# Training the model\n",
    "model_2.fit(X_train,Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=10,callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "253/255 [============================>.] - ETA: 0s - loss: 0.1765 - accuracy: 0.9293\n",
      "Epoch 00001: val_loss did not improve from 0.25852\n",
      "255/255 [==============================] - 6s 25ms/step - loss: 0.1758 - accuracy: 0.9297 - val_loss: 0.2925 - val_accuracy: 0.8712\n",
      "Epoch 2/5\n",
      "255/255 [==============================] - ETA: 0s - loss: 0.1727 - accuracy: 0.9326\n",
      "Epoch 00002: val_loss did not improve from 0.25852\n",
      "255/255 [==============================] - 6s 25ms/step - loss: 0.1727 - accuracy: 0.9326 - val_loss: 0.2873 - val_accuracy: 0.9038\n",
      "Epoch 3/5\n",
      "253/255 [============================>.] - ETA: 0s - loss: 0.1556 - accuracy: 0.9363\n",
      "Epoch 00003: val_loss did not improve from 0.25852\n",
      "255/255 [==============================] - 6s 24ms/step - loss: 0.1550 - accuracy: 0.9366 - val_loss: 0.3267 - val_accuracy: 0.8788\n",
      "Epoch 4/5\n",
      "253/255 [============================>.] - ETA: 0s - loss: 0.1533 - accuracy: 0.9370\n",
      "Epoch 00004: val_loss did not improve from 0.25852\n",
      "255/255 [==============================] - 6s 25ms/step - loss: 0.1537 - accuracy: 0.9366 - val_loss: 0.2873 - val_accuracy: 0.8769\n",
      "Epoch 5/5\n",
      "253/255 [============================>.] - ETA: 0s - loss: 0.1595 - accuracy: 0.9392\n",
      "Epoch 00005: val_loss did not improve from 0.25852\n",
      "255/255 [==============================] - 7s 26ms/step - loss: 0.1595 - accuracy: 0.9390 - val_loss: 0.2785 - val_accuracy: 0.9199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fd9924d4c8>"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling the model\n",
    "# Training the model\n",
    "model_2.fit(X_train,\n",
    "          Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=5,callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9198718070983887"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(model_2.history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1fd99274f08>"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"training_model2_stclass/cp.ckpt\"\n",
    "model_ckpt2 = model_2\n",
    "model_ckpt2.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt2.save('model_2_3stat.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#converging Loss to 0.92 is the biggest and hectic tried various model with and without LSTM ,CNN models by changing no of LSTM units and corresponding drop outs and max_poolsize finally achieved this due to less data took lot of hyperparameter tuning\n",
    "#finally converged to loss of 0.2585 and saved for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classifying dynamic activities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the train and test data\n",
    "filename = f'UCI_HAR_Dataset/test/y_test.txt'\n",
    "y = _read_csv(filename)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dyna(subset):\n",
    "    \"\"\"\n",
    "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "    that represents a human activity. We return a binary representation of \n",
    "    every sample objective as a 6 bits vector using One Hot Encoding\n",
    "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "    \"\"\"\n",
    "    filename = f'UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "    y = _read_csv(filename)[0]\n",
    "    y_ind=y<=3\n",
    "    Y=y[y_ind] #for selecting dynamic features we have taken y should be less or equal to 3\n",
    "\n",
    "    return np.array(pd.get_dummies(Y)),y_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"\n",
    "    Obtain the dataset from multiple files.\n",
    "    Returns: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test = load_signals('train'), load_signals('test')\n",
    "    y_train,xtr_dyn=load_dyna('train')\n",
    "    y_test,xte_dyn=load_dyna('test')\n",
    "    X_train_dy=X_train[xtr_dyn]\n",
    "    X_test_dy=X_test[xte_dyn]\n",
    "\n",
    "    return X_train_dy, X_test_dy, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "9\n",
      "3285\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = _count_classes(Y_train)\n",
    "\n",
    "print(timesteps)\n",
    "print(input_dim)\n",
    "print(len(X_train))\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 128, 100)          44000     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 128, 64)           19264     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 128, 25)           4825      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128, 25)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 128, 25)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 9603      \n",
      "=================================================================\n",
      "Total params: 77,692\n",
      "Trainable params: 77,692\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "n_hidden1=100\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(LSTM(n_hidden1,return_sequences=True, input_shape=(timesteps, input_dim)))\n",
    "model3.add(Dropout(0.65))\n",
    "model3.add(Conv1D(filters=64, kernel_size=3,padding='same', activation='relu',kernel_initializer='he_uniform'))\n",
    "model3.add(Dropout(0.375))\n",
    "model3.add(Conv1D(filters=25, kernel_size=3,padding='same', activation='relu',kernel_initializer='he_uniform'))\n",
    "model3.add(Dropout(0.275))\n",
    "model3.add(MaxPooling1D(pool_size=1,strides=1))\n",
    "model3.add(Flatten())\n",
    "\n",
    "#model3.add(Dense(25, activation='relu',kernel_initializer='he_uniform'))\n",
    "#model3.add(Dropout(0.275))\n",
    "\n",
    "#model3.add(Dense(32, activation='relu',kernel_initializer='he_uniform'))\n",
    "#model3.add(BatchNormalization()) \n",
    "#model3.add(Dropout(0.6))\n",
    "\n",
    "model3.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "checkpoint_path = \"training_model3_dyclass/cp.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,save_best_only=True,save_weights_only=True,verbose=1)\n",
    "\n",
    "#checkpoint_path = \"training_model3_dyclass/cp.ckpt\"\n",
    "#model_ckpt2 = model2\n",
    "#model_ckpt2.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "205/206 [============================>.] - ETA: 0s - loss: 0.6878 - accuracy: 0.6869\n",
      "Epoch 00001: val_loss improved from inf to 0.33466, saving model to training_model3_dyclass/cp.ckpt\n",
      "206/206 [==============================] - 5s 26ms/step - loss: 0.6867 - accuracy: 0.6874 - val_loss: 0.3347 - val_accuracy: 0.9120\n",
      "Epoch 2/10\n",
      "204/206 [============================>.] - ETA: 0s - loss: 0.1077 - accuracy: 0.9629\n",
      "Epoch 00002: val_loss improved from 0.33466 to 0.26773, saving model to training_model3_dyclass/cp.ckpt\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 0.1085 - accuracy: 0.9626 - val_loss: 0.2677 - val_accuracy: 0.9308\n",
      "Epoch 3/10\n",
      "205/206 [============================>.] - ETA: 0s - loss: 0.0651 - accuracy: 0.9832\n",
      "Epoch 00003: val_loss improved from 0.26773 to 0.19522, saving model to training_model3_dyclass/cp.ckpt\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 0.0650 - accuracy: 0.9833 - val_loss: 0.1952 - val_accuracy: 0.9661\n",
      "Epoch 4/10\n",
      "205/206 [============================>.] - ETA: 0s - loss: 0.0253 - accuracy: 0.9930\n",
      "Epoch 00004: val_loss did not improve from 0.19522\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 0.0253 - accuracy: 0.9930 - val_loss: 0.2929 - val_accuracy: 0.9250\n",
      "Epoch 5/10\n",
      "205/206 [============================>.] - ETA: 0s - loss: 0.0269 - accuracy: 0.9951\n",
      "Epoch 00005: val_loss did not improve from 0.19522\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 0.0269 - accuracy: 0.9951 - val_loss: 0.1998 - val_accuracy: 0.9697\n",
      "Epoch 6/10\n",
      "204/206 [============================>.] - ETA: 0s - loss: 0.0212 - accuracy: 0.9963\n",
      "Epoch 00006: val_loss did not improve from 0.19522\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 0.0210 - accuracy: 0.9963 - val_loss: 0.3889 - val_accuracy: 0.9762\n",
      "Epoch 7/10\n",
      "205/206 [============================>.] - ETA: 0s - loss: 0.0265 - accuracy: 0.9945\n",
      "Epoch 00007: val_loss improved from 0.19522 to 0.15533, saving model to training_model3_dyclass/cp.ckpt\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 0.0265 - accuracy: 0.9945 - val_loss: 0.1553 - val_accuracy: 0.9791\n",
      "Epoch 8/10\n",
      "204/206 [============================>.] - ETA: 0s - loss: 0.0127 - accuracy: 0.9985\n",
      "Epoch 00008: val_loss did not improve from 0.15533\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 0.0126 - accuracy: 0.9985 - val_loss: 0.2047 - val_accuracy: 0.9726\n",
      "Epoch 9/10\n",
      "206/206 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9967\n",
      "Epoch 00009: val_loss did not improve from 0.15533\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 0.0220 - accuracy: 0.9967 - val_loss: 0.4918 - val_accuracy: 0.9120\n",
      "Epoch 10/10\n",
      "206/206 [==============================] - ETA: 0s - loss: 0.0226 - accuracy: 0.9973\n",
      "Epoch 00010: val_loss did not improve from 0.15533\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 0.0226 - accuracy: 0.9973 - val_loss: 0.2331 - val_accuracy: 0.9589\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fd92b11808>"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling the model\n",
    "model3.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',metrics=['accuracy'])\n",
    "# Training the model\n",
    "model3.fit(X_train,Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=10,callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "205/206 [============================>.] - ETA: 0s - loss: 0.0060 - accuracy: 0.9976\n",
      "Epoch 00001: val_loss improved from 0.15533 to 0.15506, saving model to training_model3_dyclass/cp.ckpt\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 0.0060 - accuracy: 0.9976 - val_loss: 0.1551 - val_accuracy: 0.9762\n",
      "Epoch 2/5\n",
      "204/206 [============================>.] - ETA: 0s - loss: 0.0100 - accuracy: 0.9969\n",
      "Epoch 00002: val_loss did not improve from 0.15506\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 0.0099 - accuracy: 0.9970 - val_loss: 0.2241 - val_accuracy: 0.9611\n",
      "Epoch 3/5\n",
      "205/206 [============================>.] - ETA: 0s - loss: 0.0054 - accuracy: 0.9991\n",
      "Epoch 00003: val_loss improved from 0.15506 to 0.10954, saving model to training_model3_dyclass/cp.ckpt\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 0.0054 - accuracy: 0.9991 - val_loss: 0.1095 - val_accuracy: 0.9813\n",
      "Epoch 4/5\n",
      "205/206 [============================>.] - ETA: 0s - loss: 0.0245 - accuracy: 0.9966\n",
      "Epoch 00004: val_loss improved from 0.10954 to 0.02850, saving model to training_model3_dyclass/cp.ckpt\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 0.0245 - accuracy: 0.9967 - val_loss: 0.0285 - val_accuracy: 0.9942\n",
      "Epoch 5/5\n",
      "204/206 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9994\n",
      "Epoch 00005: val_loss did not improve from 0.02850\n",
      "206/206 [==============================] - 5s 23ms/step - loss: 0.0040 - accuracy: 0.9994 - val_loss: 0.1748 - val_accuracy: 0.9769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fcfb368648>"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Training the model\n",
    "model3.fit(X_train,Y_train,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          epochs=5,callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.994232177734375"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(model3.history.history['val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1fd4b714988>"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"training_model3_dyclass/cp.ckpt\"\n",
    "model3 = model3\n",
    "model3.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save('model_3_dyclass.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#this is not a big task as tried with the same model of model_2 by little change in drop outs  it worked well achieved  accuracy  of 99.4% with loss of 0.2850"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final prediction by combinations of all the three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "model1= load_model('C:/Users/thrib/Documents/HAR/model_1_2class.h5')\n",
    "model2 = load_model(\"C:/Users/thrib/Documents/HAR/model_2_3stat.h5\")\n",
    "model3 = load_model(\"C:/Users/thrib/Documents/HAR/model_3_dyclass.h5\")\n",
    "model1.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',metrics=['accuracy'])\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',metrics=['accuracy'])\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',metrics=['accuracy'])\n",
    "#saved and combined all three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_2class = model1.predict(X_test)\n",
    "pred_2class =  np.argmax(predict_2class, axis=1)\n",
    "\n",
    "X_static = X_test[pred_2class==0]#static \n",
    "X_dynamic = X_test[pred_2class==1]#dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_2class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1389, 128, 9)"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_dynamic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1558, 128, 9)"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_static.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2947"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1558+1389"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predicting stat values and using argmax to convert the probalities into int values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_stat = model2.predict(X_static)\n",
    "pred_3stclass =  np.argmax(predict_stat, axis=1)\n",
    "\n",
    "#stat 4,5,6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.2831606e-03, 9.9171680e-01, 4.7971463e-13],\n",
       "       [1.9125558e-02, 9.8087448e-01, 1.1112041e-09],\n",
       "       [2.5607893e-02, 9.7439212e-01, 1.8017881e-08],\n",
       "       ...,\n",
       "       [9.6315933e-12, 9.7943799e-19, 1.0000000e+00],\n",
       "       [5.8087567e-12, 6.0154342e-19, 1.0000000e+00],\n",
       "       [5.9103573e-12, 4.2505405e-19, 1.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_stat # these are the prob foe each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 6], dtype=int64)"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stclass=pred_3stclass+4\n",
    "np.unique(stclass)\n",
    "# as they will be 0,1,2 we are adding 4 to to get corresponding values t get 4,5,6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predicing dyna values similarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_dyna = model3.predict(X_dynamic)\n",
    "pred_3dyclass =  np.argmax(predict_dyna, axis=1)\n",
    "# 1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dyclass=pred_3dyclass+1\n",
    "np.unique(dyclass)\n",
    "# as they will be 0,1,2 we are adding 1 to to get corresponding values 1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finally appending in a sequence of the data of original test values\n",
    "i,j=0,0\n",
    "final_pred = []\n",
    "for val in pred_2class:\n",
    "    if val == 1:\n",
    "        final_pred.append(dyclass[i])\n",
    "        i = i + 1\n",
    "    else:\n",
    "        final_pred.append(stclass[j])\n",
    "        j = j + 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'UCI_HAR_Dataset/test/y_test.txt'\n",
    "y_test = _read_csv(filename)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test data 0.9514760773668137\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##accuracy of test\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy of test data',accuracy_score(y_test,final_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "| Confusion Matrix |\n",
      "--------------------\n",
      "\n",
      " [[491   5   0   0   0   0]\n",
      " [  1 468   2   0   0   0]\n",
      " [  0   0 420   0   0   0]\n",
      " [  0   0   0 414  76   1]\n",
      " [  1   1   0  56 474   0]\n",
      " [  0   0   0   0   0 537]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, final_pred,labels=range(1,7))\n",
    "print('--------------------')\n",
    "print('| Confusion Matrix |')\n",
    "print('--------------------')\n",
    "print('\\n {}'.format(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_ACTIVITIES = {1:'WALKING',2:'WALKING_UPSTAIRS',3:'WALKING_DOWNSTAIRS'}\n",
    "DYNA_ACTIVITIES = {4: 'SITTING',5: 'STANDING',6: 'LAYING'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.grid(b=False)\n",
    "plot_confusion_matrix(cm, classes=range(1,7), normalize=True, title='Normalized confusion matrix')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "x = PrettyTable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+----------+\n",
      "|            models            | Test_auc |\n",
      "+------------------------------+----------+\n",
      "|      model_single_LSTM       | 0.92235  |\n",
      "|    model_multi_layer_Lstm    |  0.9288  |\n",
      "| model_dividmconquer_Lstm_cnn | 0.95147  |\n",
      "+------------------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "x.field_names = [\"models\", \"Test_auc\"]\n",
    "x.add_row([\"model_single_LSTM\", 0.92235])\n",
    "x.add_row([\"model_multi_layer_Lstm\", 0.9288])\n",
    "x.add_row([\"model_dividmconquer_Lstm_cnn\", 0.95147])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
